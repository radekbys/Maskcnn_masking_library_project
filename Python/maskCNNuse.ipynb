{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeszyt z przykładowym użyciem maskcnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 640, 640])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(640, 640, 3)\n",
    "y = x.permute(2, 0, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@0.891] global loadsave.cpp:241 findDecoder imread_('../../rose.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m im\u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../rose.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "im= cv2.imread(\"../../rose.jpeg\")\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 991, 1280])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageTensor = torch.from_numpy(im)\n",
    "imageTensor = imageTensor.permute(2, 0, 1)\n",
    "imageTensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[   0.0000,  374.1825, 1280.0000,  991.0000],\n",
       "          [   0.0000,    0.0000, 1280.0000,  991.0000]],\n",
       "         grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([67,  1]),\n",
       "  'scores': tensor([1., 1.], grad_fn=<IndexBackward0>),\n",
       "  'masks': tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "             0.0000e+00, 0.0000e+00],\n",
       "            ...,\n",
       "            [1.3825e-03, 1.4436e-03, 1.5046e-03,  ..., 1.0891e-05,\n",
       "             1.0467e-05, 1.0042e-05],\n",
       "            [1.2870e-03, 1.3438e-03, 1.4006e-03,  ..., 1.0138e-05,\n",
       "             9.7431e-06, 9.3479e-06],\n",
       "            [1.1914e-03, 1.2440e-03, 1.2966e-03,  ..., 9.3855e-06,\n",
       "             9.0197e-06, 8.6537e-06]]],\n",
       "  \n",
       "  \n",
       "          [[[9.9883e-14, 1.0429e-13, 1.0870e-13,  ..., 1.9910e-11,\n",
       "             1.9134e-11, 1.8357e-11],\n",
       "            [1.0549e-13, 1.1015e-13, 1.1481e-13,  ..., 2.1028e-11,\n",
       "             2.0209e-11, 1.9389e-11],\n",
       "            [1.1111e-13, 1.1601e-13, 1.2092e-13,  ..., 2.2147e-11,\n",
       "             2.1284e-11, 2.0420e-11],\n",
       "            ...,\n",
       "            [1.3973e-10, 1.4590e-10, 1.5207e-10,  ..., 3.5847e-21,\n",
       "             3.4450e-21, 3.3052e-21],\n",
       "            [1.3302e-10, 1.3889e-10, 1.4476e-10,  ..., 3.4124e-21,\n",
       "             3.2794e-21, 3.1463e-21],\n",
       "            [1.2630e-10, 1.3187e-10, 1.3745e-10,  ..., 3.2400e-21,\n",
       "             3.1137e-21, 2.9874e-21]]]], grad_fn=<UnsqueezeBackward0>)}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions=model(z)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
